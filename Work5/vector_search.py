import os
import math
from collections import defaultdict
from tqdm import tqdm
from pymorphy3 import MorphAnalyzer

def load_tfidf(terms_dir):
    if not os.path.exists(terms_dir):
        print(f"[ERROR] Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ {terms_dir} Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚!")
        return

    morph = MorphAnalyzer()
    tfidf = defaultdict(dict)
    files = [f for f in os.listdir(terms_dir) if f.endswith("_terms.txt")]

    for file in tqdm(files, desc="[INFO] Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° TF-IDF", leave=False):
        doc_id = file.replace("_terms.txt", "")
        try:
            with open(os.path.join(terms_dir, file), "r", encoding="utf-8") as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) != 3:
                        continue
                    term, _, tfidf_val = parts
                    lemma = morph.parse(term)[0].normal_form
                    tfidf[doc_id][lemma] = float(tfidf_val)
        except Exception as e:
            print(f"[ERROR] ÐžÑˆÐ¸Ð±ÐºÐ° Ð² {file}: {str(e)}")
    return tfidf

def load_inverted_index(index_file):
    if not os.path.exists(index_file):
        print(f"[ERROR] Ð¤Ð°Ð¹Ð» {index_file} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
        return

    index = defaultdict(list)
    try:
        with open(index_file, "r", encoding="utf-8") as f:
            for line in f:
                if ": " not in line:
                    continue
                lemma, docs = line.strip().split(": ", 1)
                index[lemma] = [d.replace(".txt", "") for d in docs.split(", ")]
    except Exception as e:
        print(f"[ERROR] Ð¤Ð°Ð¹Ð» {index_file}: {str(e)}")
    return index

def load_links(index_file):
    if not os.path.exists(index_file):
        print(f"[ERROR] Ð¤Ð°Ð¹Ð» {index_file} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
        return

    links = defaultdict()
    try:
        with open(index_file, "r", encoding="utf-8") as f:
            for line in f:
                if " - " not in line:
                    continue
                file_name, link = line.strip().split(" - ", 2)
                links[file_name.replace(".txt", "")] = link
    except Exception as e:
        print(f"[ERROR] Ð¤Ð°Ð¹Ð» {index_file}: {str(e)}")
    return links

def lemmatize_query(query):
    morph = MorphAnalyzer()
    lemmas = []
    for word in query.split():
        parsed = morph.parse(word.lower())
        if not parsed:
            continue
        lemma = parsed[0].normal_form
        lemmas.append(lemma)
    return lemmas

def vector_search(query, tfidf, index, links, top_n=10):
    query_terms = lemmatize_query(query)
    if not query_terms:
        return []

    total_docs = len(tfidf)
    idf = {}
    for term in query_terms:
        doc_freq = len(index.get(term, []))
        idf[term] = math.log((total_docs + 1) / (doc_freq + 1e-6)) + 1

    query_tf = {term: query_terms.count(term) / len(query_terms) for term in query_terms}
    query_vec = {term: tf * idf.get(term, 0) for term, tf in query_tf.items()}

    candidates = set()
    for term in query_terms:
        candidates.update(index.get(term, []))

    results = []
    for doc in candidates:
        if doc not in tfidf:
            continue

        doc_vec = tfidf[doc]
        dot = sum(query_vec.get(term, 0) * doc_vec.get(term, 0) for term in query_terms)
        norm_query = math.sqrt(sum(v ** 2 for v in query_vec.values()))
        norm_doc = math.sqrt(sum(v ** 2 for v in doc_vec.values()))

        score = dot / (norm_query * norm_doc) if (norm_query * norm_doc) != 0 else 0.0

        link = links[doc]
        results.append((doc, score, link))

    return sorted(results, key=lambda x: -x[1])[:top_n]

def main():
    print("ðŸ” Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
    tfidf = load_tfidf("../Work4/result/terms")
    index = load_inverted_index("../Work3/result/inverted_index.txt")
    links = load_links("../Work1/result/index.txt")
    print(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(tfidf)}")

    while True:
        try:
            query = input("\nðŸ“ Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð·Ð°Ð¿Ñ€Ð¾Ñ (exit Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð°): ").strip()
            if query.lower() == 'exit':
                print("\nðŸ›‘ Ð Ð°Ð±Ð¾Ñ‚Ð° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°")
                break

            results = vector_search(query, tfidf,links, index)

            if not results:
                print("\nâŒ Ð¡Ð¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
                continue

            print("\nðŸ”Ž Ð¢Ð¾Ð¿ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²:")
            for i, (doc, score) in enumerate(results[:10], 1):
                print(f"{i:>2}. Ð”Ð¾Ðº. {doc:<5} | Ð¡Ñ…Ð¾Ð´ÑÑ‚Ð²Ð¾: {score:.4f}")

        except KeyboardInterrupt:
            print("\nðŸ›‘ ÐŸÐ¾Ð¸ÑÐº Ð¾Ñ‚Ð¼ÐµÐ½ÐµÐ½")
            break
        except Exception as e:
            print(f"\n[ERROR] {str(e)}")

if __name__ == "__main__":
    main()